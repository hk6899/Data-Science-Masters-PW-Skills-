{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab492fcf-0fa4-4234-b591-ad3ba9943d3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Decision Tree 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947c2c5a-e02e-4425-b29b-29ed5b790a1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1088f91-476d-4470-9da3-93939efd48ff",
   "metadata": {},
   "source": [
    "Q1. Describe the decision tree classifier algorithm and how it works to make predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1eff95-fb55-4b2a-8904-6f12e9e74d22",
   "metadata": {
    "tags": []
   },
   "source": [
    "The decision tree classifier is a supervised algorihm. The decision tree classifier follows the principle of trees, where a feature is selected, splitted into different internal nodes based on some condition. THE INTERNAL NODE which cannot be further splitted is called a leaf node. The process of contionous spliting of the trees until we get all leaf nodes is the primary working of the decision tree. The feature which is getting splitted is decided based on the information gain. Information gain is the basic criterion to decide whether a feature should be used to split a node or not. The feature with the optimal split i.e., the highest value of information gain at a node of a decision tree is used as the feature for splitting the node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b796c-1cee-46f6-8f8b-72cb8afcf0d6",
   "metadata": {},
   "source": [
    "Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d482b0-4551-4aca-8f00-9ad14ec1f22b",
   "metadata": {},
   "source": [
    "Entropy: The decision tree algorithm starts by calculating the entropy of the target variable. Entropy is a measure of randomness or impurity in the data. In the context of decision trees, entropy helps to quantify the uncertainty associated with the target variable.\n",
    "\n",
    "Information Gain: The algorithm then selects the feature that best splits the data into homogeneous subsets. It does this by calculating the information gain for each feature. Information gain measures the reduction in entropy that results from splitting the data based on a particular feature. The feature with the highest information gain is chosen as the root node of the tree.\n",
    "\n",
    "Splitting: Once the root node is selected, the dataset is split into subsets based on the values of the chosen feature. Each subset represents a branch of the tree.\n",
    "\n",
    "Recursive Partitioning: This process of splitting the data based on the chosen features is then applied recursively to each subset. At each step, the algorithm selects the feature that maximizes information gain and splits the data accordingly.\n",
    "\n",
    "Stopping Criteria: The recursive partitioning process continues until one of the stopping criteria is met. This could be when all instances in a node belong to the same class, when the tree reaches a maximum depth, or when the number of instances in a node falls below a certain threshold.\n",
    "\n",
    "Prediction: Once the tree is built, it can be used to make predictions on new data. To classify an instance, it traverses the tree from the root node down to a leaf node, following the decision rules at each node. The class label associated with the majority of instances in the leaf node is then assigned to the instance being classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60fcb6b-a23e-405e-b755-4a372addad7c",
   "metadata": {},
   "source": [
    "Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ea885-d996-4cb5-8aeb-932254225ed4",
   "metadata": {},
   "source": [
    "The binary classification is done by setting the target variabl values as 0 and 1, once its done, we select the feature to split on the chosen feature. Then splitting is done until it meet a stopping criteria such s max depth or no of instancs of a tree falls under a treshold. FInally after the tree is created, it goes through all the nodes of the tree for a specific input and output is calculated until the leaf node or stopping criteria condition is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b03e66-9cff-4722-8ec9-3075c7638e08",
   "metadata": {},
   "source": [
    "Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make\n",
    "predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f4e5f7-db6d-4af5-b053-2f0b37b5d747",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Binary Splits: At each step of building the decision tree, the algorithm chooses a feature and a threshold value to split the dataset into two subsets. This split creates a hyperplane perpendicular to one of the axes in the feature space. For example, if the feature space is two-dimensional, the split could be a vertical or horizontal line.\n",
    "\n",
    "Recursive Partitioning: This splitting process is applied recursively to each subset, creating further partitions in the feature space. Each split creates a new boundary that divides the space into regions corresponding to different combinations of feature values.\n",
    "\n",
    "Decision Regions: As the tree grows, the feature space becomes partitioned into decision regions, each associated with a specific class label. The decision boundaries between these regions are defined by the feature splits made by the decision tree algorithm.\n",
    "\n",
    "Prediction: To classify a new instance, you start at the root of the decision tree and follow a path down the tree based on the feature values of the instance. At each node, you compare the feature value to the threshold associated with that node. Depending on whether the value is above or below the threshold, you move to the left or right child node, respectively. This process continues until you reach a leaf node, where the majority class label of the instances in that node is assigned to the new instance.\n",
    "\n",
    "So, the decision tree algorithm essentially carves out regions in the feature space and assigns a class label to each region based on the majority class of the instances within it. This geometric intuition allows decision trees to make predictions by simply traversing the tree structure based on the feature values of the instance to be classified, making it a powerful and intuitive method for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb98f91-9d90-49e8-8f27-9966bda1e76e",
   "metadata": {},
   "source": [
    "Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\n",
    "classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76785649-85e0-4aac-b577-e39993280854",
   "metadata": {},
   "source": [
    "The confusion matrix is a table that is used to evaluate the performance of a classification model. It provides a summary of the predictions made by the model compared to the actual class labels in the dataset. The confusion matrix is particularly useful for evaluating the performance of classification models, especially in binary classification tasks. It can evaluae the perfomance of the model by using TP FP TN and FN values respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be383ae-7426-4b33-b50d-7554a01c1430",
   "metadata": {
    "tags": []
   },
   "source": [
    "Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\n",
    "calculated from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719bba1-86f1-4449-9d4d-dd9a4b742b88",
   "metadata": {},
   "source": [
    "True Positive (TP) = 900\n",
    "False Positive (FP) = 150\n",
    "True Negative (TN) = 850\n",
    "False Negative (FN) = 100\n",
    "Now, let's calculate precision, recall, and F1 score:\n",
    "\n",
    "Precision: Precision measures the accuracy of the positive predictions made by the model. It is calculated as the ratio of true positive predictions to the total number of positive predictions made by the model.\n",
    "Precision = TP / (TP + FP)\n",
    "900/1050\n",
    "​\n",
    " =0.857\n",
    "So, the precision is 0.857 or 85.7%.\n",
    "\n",
    "Recall (Sensitivity): Recall measures the ability of the model to correctly identify positive instances. It is calculated as the ratio of true positive predictions to the total number of actual positive instances in the dataset.\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "​\n",
    " =0.900\n",
    "So, the recall is 0.900 or 90%.\n",
    "\n",
    "F1 Score: The F1 score is the harmonic mean of precision and recall, providing a balance between the two metrics. It is calculated as:\n",
    "F1_Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "​\n",
    " =0.878\n",
    "So, the F1 score is 0.878.\n",
    "\n",
    "These metrics provide a comprehensive evaluation of the model's performance. In this example, the model has high precision, indicating that when it predicts a positive outcome, it is correct most of the time. Additionally, the model has high recall, indicating that it can identify a large proportion of positive instances in the dataset. Finally, the F1 score provides a balance between precision and recall, taking into account both false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ab9b00-1f75-4a7d-b88e-08409578a0c6",
   "metadata": {},
   "source": [
    "Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and\n",
    "explain how this can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d017dfd8-157c-499c-ae9b-8dfa430922f7",
   "metadata": {},
   "source": [
    " choosing the right evaluation metric for a classification problem is crucial because it determines how well the model's performance aligns with the goals of the task. It's important to consider factors like class imbalance, trade-offs between metrics, and stakeholder priorities. By understanding the problem domain, dataset characteristics, and the trade-offs involved, you can select the most appropriate metric to accurately assess model performance and make informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3573c9b-aabd-439e-b348-96b127157136",
   "metadata": {},
   "source": [
    "Q8. Provide an example of a classification problem where precision is the most important metric, and\n",
    "explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c64e50c-c08c-42e8-aefb-8df6c31621f6",
   "metadata": {},
   "source": [
    "Precision = TP / (TP + FP). What proportion of positive identifications was actually correct .\n",
    " Eg: Spam classification. So precision considers the false positives as well. If a mail which is not a spam but classified as a spam mail will not create a huge impact on real world scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654d483-ac2e-4b52-8d30-46eb43836305",
   "metadata": {},
   "source": [
    "Q9. Provide an example of a classification problem where recall is the most important metric, and explain\n",
    "why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a29902-3110-4c36-8282-5b9fad62cf46",
   "metadata": {},
   "source": [
    "Recall = TP/TP+FN. What Proportion of actual positive are identified correctly. \n",
    "Eg: Cancer Prediction. So if a person has cancer but predicted as non cancer can be danger. So FN has to be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6920c5-9d63-46db-aa4d-de49453c6e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e9b44-476d-4ed8-9c1b-79c415fad007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee055c-f40a-43b8-984f-c14cf11031bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04f20ed-eb27-4f3e-b264-c6fc683d6787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2628c7e-68bf-402c-b211-3c4d10f482b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa57860-979e-4f90-ab54-090c4473402b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2304e40c-645b-4c9c-a84b-57663590de8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a9c72-614d-449d-a03d-27eae41bd893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309fd22-0c0d-4645-8d56-ab1c9f96321c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04de604-97d8-4ffe-906d-1595a4c0dca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189e587-b779-434b-bced-0ca4cb151cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c382dd5-e79d-4b24-83c7-574eaff62c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce89a5fa-15ac-4e82-966f-70924cf3eb32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c0394-9126-4299-a7b2-846e9e1f7914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c38cce-977e-44ab-aa5d-6fcb6246cef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92aa34c-3abe-4127-9d8f-1b5efa04d2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2068d6-588d-45e4-8e40-d7f08669a828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84d419-bf9e-4e4b-b167-21c3acaf4030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd3cf28-4720-4635-8900-51b1d9b9d22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17607179-7314-4e34-ab05-ad505eea4551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0698696-15ee-4871-b843-3f7e97e68164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3917c9-7190-47a7-b3c9-6edd3e342c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6dd782-537d-4747-8316-638ca3ef6dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28abb7a8-8511-4af4-90aa-c450071acaba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d330de-8711-4b8c-8edf-63264d1424ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7aed27-bc50-4cb7-a21e-1239651aa3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9942a9-8336-40b8-8e97-3578ee2c6ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bdcf60-076f-4936-bb6f-85b215033c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
