{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c380c0-5037-40e0-a516-a49580793ce9",
   "metadata": {},
   "source": [
    "# Regression 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933c6089-bcb9-42fc-83c2-7ee08df768b0",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b1b1e-ac30-4dcb-8da0-80a058782e6f",
   "metadata": {},
   "source": [
    "Lasso is also called L1 regularisation. It adds a penality to the normal regression cost function, helps in prevent overfitting and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9881adc6-e9d1-48af-af9b-c3903f145209",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5986d4c-1634-4d0f-97c4-68d31bf4775e",
   "metadata": {},
   "source": [
    "When hyperparameter is changed for different features, the one with the least importance will have 0 as the global minima, which can be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efccd3a1-c102-4bf1-805c-69a0592f7aae",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c30c5-25fa-4706-a454-8f21729ea077",
   "metadata": {},
   "source": [
    "Non-Zero Coefficients:\n",
    "\n",
    "If the coefficient of a particular feature is non-zero, it means that the Lasso Regression model considers that feature to be relevant in predicting the target variable.\n",
    "A positive coefficient indicates a positive relationship between the feature and the target variable, while a negative coefficient indicates a negative relationship.\n",
    "Zero Coefficients:\n",
    "\n",
    "If the coefficient of a feature is exactly zero, it means that Lasso has effectively performed feature selection, and that particular feature does not contribute to the model.\n",
    "Features with zero coefficients have been deemed less important by the Lasso algorithm and are excluded from the final prediction.\n",
    "Magnitude of Non-Zero Coefficients:\n",
    "\n",
    "The magnitude of the non-zero coefficients indicates the strength of the relationship between each feature and the target variable.\n",
    "Larger magnitude coefficients suggest a stronger impact on the target variable, and smaller magnitude coefficients suggest a weaker impact.\n",
    "Sparsity in Model:\n",
    "\n",
    "Lasso Regression is known for its ability to create sparse models by setting some coefficients to exactly zero.\n",
    "The sparsity in the model helps with feature selection, making the model more interpretable and potentially improving its generalization to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec2ec2c-fc78-4b72-a1ae-dccc61f7972a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db8d8a-1693-4bcb-810f-5ffd59e28a0f",
   "metadata": {},
   "source": [
    "Regularization Parameter (Lambda/Alpha):\n",
    "\n",
    "Purpose: Controls the amount of regularization applied to the model.\n",
    "Effect on Model Performance:\n",
    "Smaller values of lambda result in weaker regularization, allowing the model to fit the training data more closely. However, this may lead to overfitting, especially in the presence of multicollinearity.\n",
    "Larger values of lambda increase the regularization strength, promoting sparsity in the coefficient values. This helps in feature selection and prevents overfitting.\n",
    "\n",
    "Solver Algorithm:\n",
    "\n",
    "Purpose: The solver algorithm determines the optimization approach used to find the coefficients that minimize the cost function.\n",
    "Effect on Model Performance:\n",
    "Different solvers may have different computational efficiencies and may be more suitable for certain types of datasets or problems.\n",
    "Popular solvers include coordinate descent, which is efficient for large datasets, and least angle regression (LARS), which is useful when dealing with high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd771cef-909b-4faf-a776-70bc9a4bb992",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acee99e-f2a8-48dc-b8b4-a0dddb3a9ed9",
   "metadata": {},
   "source": [
    "\n",
    "Lasso Regression, like standard linear regression, is inherently a linear modeling technique. It is specifically designed for linear relationships between the features and the target variable. However, with some modifications and extensions, Lasso principles can be applied to address non-linear regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e2b52-8da5-4e68-9540-79f38fd5ec97",
   "metadata": {
    "tags": []
   },
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192106c-8bc1-4526-9db5-edf11e391deb",
   "metadata": {},
   "source": [
    "Regularization Term:\n",
    "\n",
    "Ridge Regression: It adds a regularization term to the OLS cost function, commonly known as the L2 penalty. The L2 penalty is the sum of the squared magnitudes of the coefficients multiplied by a regularization parameter (lambda or alpha).\n",
    "Lasso Regression: It uses a different regularization term, known as the L1 penalty. The L1 penalty is the sum of the absolute values of the coefficients multiplied by a regularization parameter (lambda or alpha).\n",
    "\n",
    "Effect on Coefficients:\n",
    "\n",
    "Ridge Regression: The L2 penalty in Ridge Regression shrinks the coefficients towards zero but does not force them to be exactly zero. It tends to distribute the weight among all features, even if some of them are less relevant. Ridge Regression is effective in handling multicollinearity.\n",
    "Lasso Regression: The L1 penalty in Lasso Regression has a sparsity-inducing property. It tends to shrink some coefficients exactly to zero, effectively performing feature selection. Lasso is useful when there is a suspicion that many features are irrelevant or when dealing with sparse datasets.\n",
    "\n",
    "Feature Selection:\n",
    "\n",
    "Ridge Regression: Does not perform feature selection in the sense of setting coefficients to exactly zero. It may shrink coefficients close to zero but retains all features in the model.\n",
    "Lasso Regression: Can lead to feature selection by setting the coefficients of less important features to zero. It automatically selects a subset of features, discarding the less relevant ones.\n",
    "\n",
    "Mathematical Formulation:\n",
    "\n",
    "Ridge Regression: The objective function includes the sum of squared coefficients multiplied by the L2 penalty term. The goal is to minimize the sum of squared residuals and the regularization term.\n",
    "Lasso Regression: The objective function includes the sum of the absolute values of coefficients multiplied by the L1 penalty term. The goal is to minimize the sum of squared residuals and the regularization term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d25ac8-7e07-46ea-8b46-ac70a2ab441f",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce501502-80ba-4507-bf09-beb07ab166ac",
   "metadata": {},
   "source": [
    "Lasso Regression can be a useful tool for handling multicollinearity by automatically selecting a subset of important features and setting the coefficients of less important (correlated) features to zero. However, the effectiveness of Lasso in handling multicollinearity depends on the specific characteristics of the dataset and the strength of the multicollinearity. The L1 penalty has a useful property for handling multicollinearity: it tends to shrink some coefficients to exactly zero. This means that Lasso Regression can automatically select a subset of important features and discard the less important ones, effectively addressing multicollinearity by eliminating redundant variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c4d947-e401-40c5-a20a-c05c14b76ffa",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d0c3c0-2fe1-4a67-acb4-4a9c84a9af93",
   "metadata": {},
   "source": [
    "Cross-Validation:\n",
    "\n",
    "Split your dataset into training and validation sets.\n",
    "Train Lasso Regression models with different values of lambda on the training set.\n",
    "Evaluate each model's performance on the validation set using a metric like mean squared error (MSE) or cross-validated R-squared.\n",
    "Choose the lambda that gives the best performance on the validation set.\n",
    "Grid Search:\n",
    "\n",
    "Define a range of lambda values to explore.\n",
    "Train Lasso Regression models with each lambda value on the training set.\n",
    "Evaluate the models on a validation set.\n",
    "Select the lambda that yields the best performance.\n",
    "Randomized Search:\n",
    "\n",
    "Similar to grid search, but instead of evaluating every possible lambda value, randomly sample a subset of values from a predefined range.\n",
    "This can be more computationally efficient and still provide good results.\n",
    "Regularization Path:\n",
    "\n",
    "The regularization path is a plot of the coefficients against the regularization parameter. It can be helpful in visualizing how the coefficients change as lambda varies.\n",
    "Plot the regularization path and look for the point where coefficients start becoming zero (the elbow point). This could be a good indicator of the optimal lambda.\n",
    "Information Criteria:\n",
    "\n",
    "Information criteria like AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) can be used to assess model fit while penalizing for the number of features.\n",
    "Choose the lambda that minimizes the information criterion.\n",
    "Automatic Methods:\n",
    "\n",
    "Some algorithms and libraries provide automatic methods for choosing the optimal lambda, such as scikit-learn's LassoCV and LassoLarsCV.\n",
    "These methods internally perform cross-validation and select the lambda that minimizes the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794e60b-66fb-471c-a809-5754c0bc063e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa28b98-57c3-4fa2-b6dd-80e53082d3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d8e56-4739-42de-846c-562275d842ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38330682-abb2-4cd0-93c9-482a76ccdfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecec8e2-de68-41bb-bf1a-670a6d73460c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bfb79b-706f-486e-be25-cf59d6f61168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc262b-8a86-40bd-b0ef-c9770ca17492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769239e-029a-4683-9be6-4501bf959377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff54a5-1536-4796-9fb4-e8ff45bc9d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f99420-e674-421c-9b88-64504a742cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff5b051-b7bf-41f4-9c10-16310966e47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b1c57-08d4-4f39-8c84-965264a9c561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f81459-3218-484a-9537-1f1d61d28f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7b548-45d8-4ded-a319-f913a41343fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066b2b3f-7b35-4144-8ec2-e84f7c8430d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b29cd0-f3b9-4b10-817d-2f1125109817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9643dd-cfe3-4a97-8929-243523f640e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe02a9a-9056-4d17-8107-4f7c216dee01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f317d02-413c-44f1-84fe-8a58724fd0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13220422-d61a-46ab-93fd-d08e0a90e5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a1200-d93f-48c9-8047-ba9d9c8f4387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0bd40b-8520-4cf6-bb6d-bea2dfd3311d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d1d2b-3cec-49bd-9f8a-ba5faa623170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086cb19-61f9-43b5-ada9-be694787946e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea8f51-0761-45a6-95c5-ee7efea7b3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1329dbd-08a6-4879-be7e-e78f7181e4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a91925-744a-48e8-a243-40017497e912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a5451f-db4b-498f-ba00-b95b288fdffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f71d60-fa43-4fee-9851-44e8b4da0749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40419ab-0f98-4585-9b3f-5f441e9a6c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98add0-73b1-4d2d-bea4-eb0e0c037924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2988486-48e3-4259-b75c-63f021bdbf98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c2425-8af0-40b0-a2ea-a988464cc57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad8cf6-b482-4d2b-aa13-7357f4d15846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce002b-65d8-43de-830c-a592aeb5f470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1738fa36-9da5-41f3-ba39-f0b9ae9c8929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97131fc0-1a09-4bb4-96ee-75ba1fdb2f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed1bd05-e86c-454c-baf2-7c4b8ee09c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2aecf6-d81d-463b-ab08-b6a12454c173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa8329-76a4-48ce-b50f-f8597271326f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d000f4f-22c5-4c59-a596-bae57917663f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
